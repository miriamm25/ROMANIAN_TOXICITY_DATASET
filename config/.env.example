# TORCH-RaR Environment Configuration
# Copy this file to .env and fill in your values:
#   cp .env.example .env

# =============================================================================
# Required: OpenRouter API (for cloud inference)
# =============================================================================
# Get your API key from: https://openrouter.ai/keys
OPENROUTER_API_KEY=your_openrouter_api_key_here

# =============================================================================
# vLLM Configuration (for local GPU inference)
# =============================================================================
# Model to load in vLLM (must be compatible with vLLM)
VLLM_MODEL=meta-llama/Llama-3.1-8B-Instruct

# Number of GPUs for tensor parallelism (1 for single GPU)
VLLM_TENSOR_PARALLEL=1

# Maximum context length (reduce if running out of GPU memory)
VLLM_MAX_MODEL_LEN=8192

# GPU memory utilization ratio (0.0-1.0)
VLLM_GPU_MEMORY_UTILIZATION=0.9

# =============================================================================
# HuggingFace Configuration (for downloading gated models)
# =============================================================================
# Required for gated models like Llama
# Get your token from: https://huggingface.co/settings/tokens
HUGGING_FACE_HUB_TOKEN=

# =============================================================================
# LiteLLM Proxy Configuration
# =============================================================================
# Master key for LiteLLM proxy authentication
LITELLM_MASTER_KEY=sk-1234
