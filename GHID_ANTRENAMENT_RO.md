# Ghid Antrenament Dataset Toxicitate - Politicieni RomÃ¢ni

## âš ï¸ IMPORTANT: Despre llama.cpp

**llama.cpp NU este potrivit pentru antrenament!**

- llama.cpp este optimizat pentru **INFERENÈšÄ‚** (rulare modele cuantizate)
- NU suportÄƒ fine-tuning sau antrenament
- Este util doar pentru rularea rapidÄƒ a modelelor antrenate, nu pentru antrenare

**Pentru antrenament ai nevoie de:**
- PyTorch + Transformers (deja instalat Ã®n proiect)
- CUDA/GPU pentru antrenament eficient
- TRL (Training Reinforcement Learning) - deja Ã®n dependenÈ›e

---

## ğŸ¯ Modele Recomandate pentru Antrenament Ã®n Limba RomÃ¢nÄƒ

### 1. **DeepSeek-R1-Distill-Qwen-7B** (CURRENT - BUN)
- âœ… Deja configurat Ã®n proiect
- âœ… Suport bun pentru romÃ¢nÄƒ
- âœ… Model de raÈ›ionament (reasoning)
- âœ… 7B parametri - eficient pentru GPU

### 2. **Qwen2.5-7B-Instruct** (RECOMANDAT)
- âœ… Excelent suport multilingv (inclusiv romÃ¢nÄƒ)
- âœ… Model instruct optimizat
- âœ… PerformanÈ›Äƒ bunÄƒ pe task-uri de clasificare
- âœ… Disponibil pe HuggingFace: `Qwen/Qwen2.5-7B-Instruct`

### 3. **OpenLLM-Ro** (SPECIFIC ROMÃ‚NÄ‚)
- âœ… Dezvoltat de Politehnica BucureÈ™ti
- âœ… Antrenat pe milioane de documente romÃ¢neÈ™ti
- âš ï¸ Trebuie verificat disponibilitatea pe HuggingFace
- ğŸ”— CÄƒutare: `ai-romania` sau `OpenLLM-Ro` pe HuggingFace

### 4. **Llama 3.1 8B Instruct** (ALTERNATIVÄ‚)
- âœ… Suport multilingv bun
- âœ… Model instruct robust
- âœ… Disponibil: `meta-llama/Llama-3.1-8B-Instruct`

---

## ğŸ“Š Structura Dataset-ului Actual

Din fiÈ™ierul `judge_reasoning.jsonl` vÄƒd cÄƒ ai:
- Texte Ã®n limba romÃ¢nÄƒ despre politicieni
- EvaluÄƒri de la judge (rating 1-10)
- CompletÄƒri de la model pentru clasificare toxic/non-toxic

**Format actual:**
```json
{
  "call": 1,
  "rating": 5,
  "judge_response": "",
  "original_text": "text Ã®n romÃ¢nÄƒ...",
  "model_completion": "rÄƒspuns model..."
}
```

---

## ğŸš€ PaÈ™i pentru Augmentare Dataset

### Pasul 1: VerificÄƒ Dataset-ul Augmentat ExistÄƒ

```bash
cd /home/miriam/torch_rar_project
uv run python3 -c "import pandas as pd; df = pd.read_parquet('output/augmented_dataset.parquet'); print(f'Dataset: {len(df)} samples'); print(f'Columns: {list(df.columns)}')"
```

### Pasul 2: RuleazÄƒ Augmentare (dacÄƒ nu existÄƒ sau vrei mai mult)

```bash
# Augmentare cu 100 de sample-uri
uv run python main.py run --limit 100

# Sau cu rubrics predefinite (mai rapid)
uv run python main.py run --limit 100 --predefined-rubrics

# Sau doar implicit reward (mai rapid)
uv run python main.py run --limit 100 --reward-method implicit
```

### Pasul 3: VerificÄƒ Output-ul

Dataset-ul augmentat va fi salvat Ã®n:
- `output/augmented_dataset.parquet` (format recomandat)
- Sau `output/augmented_dataset.json`

---

## ğŸ“ Antrenament Model

### OpÈ›iunea 1: Antrenament cu Modelul Actual (DeepSeek-R1)

```bash
# Antrenament de bazÄƒ (2 epochs, hybrid reward)
CUDA_VISIBLE_DEVICES=0 uv run python scripts/train.py

# Antrenament cu mai multe epochs
CUDA_VISIBLE_DEVICES=0 uv run python scripts/train.py \
    --epochs 3 \
    --lr 1e-5 \
    --batch-size 2

# Antrenament cu rule-based reward (mai rapid, fÄƒrÄƒ judge)
CUDA_VISIBLE_DEVICES=0 uv run python scripts/train.py \
    --reward-mode rule_based \
    --epochs 2
```

### OpÈ›iunea 2: Antrenament cu Qwen2.5 (RECOMANDAT pentru romÃ¢nÄƒ)

```bash
CUDA_VISIBLE_DEVICES=0 uv run python scripts/train.py \
    --base-model Qwen/Qwen2.5-7B-Instruct \
    --epochs 3 \
    --lr 5e-6 \
    --reward-mode hybrid
```

### OpÈ›iunea 3: Antrenament cu Llama 3.1

```bash
CUDA_VISIBLE_DEVICES=0 uv run python scripts/train.py \
    --base-model meta-llama/Llama-3.1-8B-Instruct \
    --epochs 3 \
    --lr 5e-6
```

### Configurare GPU

Proiectul este configurat pentru 2 GPU-uri:
- **GPU 0**: Antrenament (model + LoRA + optimizer)
- **GPU 1**: Judge (DeepSeek-R1:70b via Ollama)

DacÄƒ ai doar 1 GPU:
```bash
# FoloseÈ™te doar rule-based reward (nu necesitÄƒ judge)
CUDA_VISIBLE_DEVICES=0 uv run python scripts/train.py \
    --reward-mode rule_based \
    --epochs 2
```

---

## ğŸ“ˆ Evaluare Model

### Evaluare Model Antrenat

```bash
# EvalueazÄƒ modelul final antrenat
uv run python scripts/evaluate.py --model ./checkpoints/final

# EvalueazÄƒ un checkpoint specific
uv run python scripts/evaluate.py --model ./checkpoints/checkpoint-250

# EvalueazÄƒ cu mai puÈ›ine sample-uri (test rapid)
uv run python scripts/evaluate.py --model ./checkpoints/final --max-samples 20
```

### Comparare Baseline vs Antrenat

```bash
# ComparÄƒ modelul antrenat cu baseline-ul
uv run python scripts/evaluate.py \
    --model ./checkpoints/final \
    --compare-baseline deepseek-ai/DeepSeek-R1-Distill-Qwen-7B
```

### Evaluare Baseline (model neantrenat)

```bash
uv run python scripts/evaluate.py \
    --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B \
    --baseline
```

---

## ğŸ”§ Configurare pentru Limba RomÃ¢nÄƒ

### VerificÄƒ ConfiguraÈ›ia ActualÄƒ

FiÈ™ierul `config/settings.yaml` este deja configurat pentru:
- Dataset romÃ¢nesc: `olimpia20/toxicity-dataset-ro-master`
- Prompt-uri pentru context romÃ¢nesc
- Rubrics specifice pentru discurs politic romÃ¢nesc

### Prompt-uri Personalizate

Prompt-urile sunt Ã®n `prompts/toxicity/`:
- `rubric_system.jinja2` - System prompt pentru generare rubrics
- `rubric_user.jinja2` - User prompt pentru rubrics
- `implicit_eval_system.jinja2` - System prompt pentru evaluare

Toate sunt deja optimizate pentru:
- âœ… Limba romÃ¢nÄƒ
- âœ… Context politic romÃ¢nesc
- âœ… Politicieni romÃ¢ni (Iohannis, Ciolacu, PSD, AUR, etc.)

---

## ğŸ“‹ Checklist Antrenament

- [ ] VerificÄƒ cÄƒ dataset-ul augmentat existÄƒ (`output/augmented_dataset.parquet`)
- [ ] VerificÄƒ GPU disponibil: `nvidia-smi`
- [ ] Alege modelul pentru antrenament (recomandat: Qwen2.5 sau DeepSeek-R1)
- [ ] RuleazÄƒ antrenament: `scripts/train.py`
- [ ] MonitorizeazÄƒ progresul (checkpoints Ã®n `checkpoints/`)
- [ ] EvalueazÄƒ modelul: `scripts/evaluate.py`
- [ ] ComparÄƒ cu baseline pentru a vedea Ã®mbunÄƒtÄƒÈ›iri

---

## ğŸ¯ RecomandÄƒri Finale

### Pentru Cel Mai Bun Rezultat Ã®n RomÃ¢nÄƒ:

1. **Model**: `Qwen/Qwen2.5-7B-Instruct` sau `DeepSeek-R1-Distill-Qwen-7B`
2. **Dataset**: AugmenteazÄƒ cel puÈ›in 500-1000 de sample-uri
3. **Antrenament**: 
   - 3 epochs
   - Learning rate: 5e-6
   - Hybrid reward (rule-based + judge)
   - LoRA (eficient, pÄƒstreazÄƒ modelul original)

### Pentru Antrenament Rapid (Test):

1. **Model**: `DeepSeek-R1-Distill-Qwen-7B` (deja configurat)
2. **Dataset**: 100-200 sample-uri augmentate
3. **Antrenament**:
   - 2 epochs
   - Rule-based reward (mai rapid)
   - LoRA

---

## â“ FAQ

**Q: Pot folosi llama.cpp pentru antrenament?**
A: NU. llama.cpp este doar pentru inferenÈ›Äƒ. Pentru antrenament foloseÈ™te PyTorch/Transformers.

**Q: Care model e cel mai bun pentru romÃ¢nÄƒ?**
A: Qwen2.5-7B-Instruct sau OpenLLM-Ro (dacÄƒ e disponibil).

**Q: CÃ¢t timp dureazÄƒ antrenamentul?**
A: Depinde de GPU È™i numÄƒrul de sample-uri. Pe H200: ~2-4 ore pentru 1000 sample-uri, 3 epochs.

**Q: Pot antrena fÄƒrÄƒ GPU?**
A: Teoretic da, dar va fi foarte lent. Recomand GPU cu cel puÈ›in 16GB VRAM.

---

## ğŸ“š Resurse

- Dataset: `olimpia20/toxicity-dataset-ro-master` pe HuggingFace
- Model actual: `deepseek-ai/DeepSeek-R1-Distill-Qwen-7B`
- DocumentaÈ›ie: `docs/README_TECHNICAL.md`

