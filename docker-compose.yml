version: '3.8'

services:
  # vLLM Server for local inference (GGUF model support)
  # See: https://docs.vllm.ai/en/stable/deployment/docker/
  vllm:
    image: vllm/vllm-openai:latest
    container_name: torch-rar-vllm
    runtime: nvidia
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HUGGING_FACE_HUB_TOKEN:-}
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
      - ~/.cache/gguf:/models:ro
    ports:
      - "8000:8000"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    command: >
      --model ${VLLM_MODEL:-/models/qwen2.5-7b-instruct-q3_k_m.gguf}
      --tokenizer ${VLLM_TOKENIZER:-Qwen/Qwen2.5-7B-Instruct}
      --tensor-parallel-size ${VLLM_TENSOR_PARALLEL:-1}
      --max-model-len ${VLLM_MAX_MODEL_LEN:-12384}
      --gpu-memory-utilization ${VLLM_GPU_MEMORY_UTILIZATION:-0.85}
      --host 0.0.0.0
      --port 8000
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s

  # LiteLLM Proxy Server (optional - for unified API)
  # See: https://docs.litellm.ai/docs/proxy/deploy
  litellm-proxy:
    image: ghcr.io/berriai/litellm:main-latest
    container_name: torch-rar-litellm
    ports:
      - "4000:4000"
    volumes:
      - ./config/litellm_config.yaml:/app/config.yaml
    environment:
      - OPENROUTER_API_KEY=${OPENROUTER_API_KEY:-}
      - LITELLM_MASTER_KEY=${LITELLM_MASTER_KEY:-sk-1234}
    command: --config /app/config.yaml --detailed_debug
    depends_on:
      vllm:
        condition: service_healthy
    profiles:
      - with-proxy

volumes:
  huggingface-cache:
